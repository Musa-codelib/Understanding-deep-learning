{"cells":[{"cell_type":"markdown","metadata":{"id":"bNDJx39Z35Bz"},"source":["**Importing libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1697466028334,"user":{"displayName":"Muhammad Musa","userId":"09955369669403023080"},"user_tz":-300},"id":"QrMgpKOb2-ME","outputId":"dd97ea3f-9072-4fe4-fbff-ed63710eb700"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to C:\\Users\\Global\n","[nltk_data]     Village\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n","[nltk_data] Downloading package punkt to C:\\Users\\Global\n","[nltk_data]     Village\\AppData\\Roaming\\nltk_data...\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Download necessary NLTK data\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"uM7GtYqU4Msy"},"source":["**List of sentiment sentences**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOnCNJ_O3CeL"},"outputs":[],"source":["# Sample list of new sentences\n","new_sentences = [\n","    \"I love this product. It's amazing!\",\n","    \"The customer service was terrible.\",\n","    \"The weather is beautiful today.\",\n","    \"This book is so boring.\",\n","    \"I'm not sure how I feel about this movie.\",\n","]"]},{"cell_type":"markdown","metadata":{"id":"dOcMyr-l4VsS"},"source":["**Pre-processing: Tokenization**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":437,"status":"ok","timestamp":1697466071992,"user":{"displayName":"Muhammad Musa","userId":"09955369669403023080"},"user_tz":-300},"id":"mPAdAEhB3KpD","outputId":"735e1e7f-fe5e-43ee-ac36-59bb2b8c6706"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenized Sentence 1: ['I', 'love', 'this', 'product', '.', 'It', \"'s\", 'amazing', '!']\n","Tokenized Sentence 2: ['The', 'customer', 'service', 'was', 'terrible', '.']\n"]}],"source":["# Tokenization\n","tokenized_sentences = [word_tokenize(sentence) for sentence in new_sentences]\n","\n","# Print some tokenized data\n","for i, sentence in enumerate(tokenized_sentences):\n","    if i < 2:  # Print the first two tokenized sentences as an example\n","        print(f\"Tokenized Sentence {i + 1}: {sentence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1697466899496,"user":{"displayName":"Muhammad Musa","userId":"09955369669403023080"},"user_tz":-300},"id":"qRGwD5ZO6S1Z","outputId":"8f8da545-0b45-4f37-c9ef-88f41b0ff097"},"outputs":[{"data":{"text/plain":["[['I', 'love', 'this', 'product', '.', 'It', \"'s\", 'amazing', '!'],\n"," ['The', 'customer', 'service', 'was', 'terrible', '.'],\n"," ['The', 'weather', 'is', 'beautiful', 'today', '.'],\n"," ['This', 'book', 'is', 'so', 'boring', '.'],\n"," ['I', \"'m\", 'not', 'sure', 'how', 'I', 'feel', 'about', 'this', 'movie', '.']]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_sentences"]},{"cell_type":"markdown","metadata":{"id":"2ghpmrQm4fAx"},"source":["**Pre-processing: Removal of stop words/punctuation marks**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1697466088227,"user":{"displayName":"Muhammad Musa","userId":"09955369669403023080"},"user_tz":-300},"id":"p2LSteLo3MdT","outputId":"6bf1d259-4ff5-4037-cffc-6d92624c806c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preprocessed Sentence 1: love product amazing\n","Preprocessed Sentence 2: customer service terrible\n"]}],"source":["# Remove stop words and punctuation\n","stop_words = set(stopwords.words('english'))\n","preprocessed_sentences = [[word.lower() for word in words if word.isalnum() and word.lower() not in stop_words] for words in tokenized_sentences]\n","\n","# Print the preprocessed sentences (the first two as an example)\n","for i, sentence in enumerate(preprocessed_sentences):\n","    if i < 2:  # Print the first two preprocessed sentences as an example\n","      print(f\"Preprocessed Sentence {i + 1}: {' '.join(sentence)}\")"]},{"cell_type":"markdown","metadata":{"id":"9UtbTN4S408q"},"source":["**Model training and prediction**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25000,"status":"ok","timestamp":1697466241078,"user":{"displayName":"Muhammad Musa","userId":"09955369669403023080"},"user_tz":-300},"id":"8PfDPjLJ3rRr","outputId":"e7ff8899-d2c1-437b-e1f8-29a38d85906f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Define the number of training epochs\n","epochs = 3\n","\n","# Initialize tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 labels: positive and negative\n","\n","# Convert data to input format\n","input_ids = []\n","attention_masks = []\n","\n","for sentence in preprocessed_sentences:\n","    encoded = tokenizer.encode_plus(\n","        text=sentence,\n","        add_special_tokens=True,\n","        max_length=64,\n","        padding='max_length',\n","        return_attention_mask=True,\n","        return_tensors='pt'\n","    )\n","    input_ids.append(encoded['input_ids'])\n","    attention_masks.append(encoded['attention_mask'])\n","\n","# Convert to PyTorch tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor([0, 1, 0, 1, 0])  # 0 for positive, 1 for negative (example labels)\n","\n","# Create a DataLoader for batch processing\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","dataloader = DataLoader(dataset, batch_size=4)\n","\n","# Define optimizer and loss function\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Training loop\n","model.train()\n","for epoch in range(epochs):\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        input_ids, attention_masks, labels = batch\n","        output = model(input_ids, attention_mask=attention_masks, labels=labels)\n","        loss = output.loss\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"bgKeaQtI48Gq"},"source":["**Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1581,"status":"ok","timestamp":1697466251200,"user":{"displayName":"Muhammad Musa","userId":"09955369669403023080"},"user_tz":-300},"id":"JaWop4C80vov","outputId":"da95b2e9-753d-47fa-ae24-22e871e6a489"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 80.00%\n"]}],"source":["# Evaluation\n","model.eval()\n","with torch.no_grad():\n","    predictions = []\n","    for batch in dataloader:\n","        input_ids, attention_masks, _ = batch\n","        outputs = model(input_ids, attention_mask=attention_masks)\n","        predicted_labels = torch.argmax(outputs.logits, dim=1)\n","        predictions.extend(predicted_labels.tolist())\n","\n","# Define a threshold for classifying sentiments (e.g., 0.5 for positive/negative)\n","threshold = 0.5\n","sentiment_labels = [\"positive\" if pred == 0 else \"negative\" for pred in predictions]\n","\n","# Ground truth labels for your new sentences (assuming you have them)\n","ground_truth_labels = [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\"]\n","\n","# Compare predicted sentiment labels to ground truth labels\n","correct_predictions = [1 if predicted == truth else 0 for predicted, truth in zip(sentiment_labels, ground_truth_labels)]\n","\n","# Calculate accuracy\n","accuracy = sum(correct_predictions) / len(correct_predictions)\n","print(f\"Accuracy: {accuracy:.2%}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElOjht_30wo8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcncBLZR1W7L"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO9Xh6hNvzUtbb6tL+rrrdv","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
